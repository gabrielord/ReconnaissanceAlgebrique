{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data, Batch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from generate_dataset import generate_dataset\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_DATASET_SIZE = 130\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_geometric_representation(in_graph_dict: dict, encoder) -> Data:\n",
    "    node_list = []\n",
    "    edge_mappings = []\n",
    "    def traverse_graph(graph = in_graph_dict):\n",
    "        nonlocal node_list\n",
    "        nonlocal edge_mappings\n",
    "        curr_node_index = len(node_list) - 1\n",
    "        encoded_data = encoder(graph[\"val\"])\n",
    "        node_list.append(encoded_data)\n",
    "        if hasattr(graph,\"children\"):\n",
    "            for child in graph[\"children\"]:\n",
    "                edge_mappings.append((graph[\"id\"], traverse_graph(child)) )\n",
    "        return curr_node_index\n",
    "    traverse_graph()\n",
    "    nodes = torch.tensor(node_list,dtype=torch.float32)\n",
    "    edges = torch.tensor([[x[0] for x in edge_mappings], [x[1] for x in edge_mappings]], dtype=torch.long) # Probably slow and mentally degenerated\n",
    "    geom_data = Data(x=nodes, edge_index=edges)\n",
    "    return geom_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPERATIONS = [\"ADD\", \"MUL\", \"FUNC\", \"POW\"]\n",
    "FUNCTIONS = [\"SIN\", \"COS\", \"TAN\", \"EXP\", \"LOG\", \"f\", \"g\", \"h\"]\n",
    "ATOMICS = [\"LITERAL\", \"VARIABLE\"]\n",
    "VARIABLE_ALPHABET = [chr(x) for x in range(ord(\"a\"), ord(\"z\")+1) if chr(x) not in [\"f\", \"g\", \"h\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_node_attribute_encoder(label_encoder:LabelEncoder, rep = 3):\n",
    "    def node_attr_encoder(attr):\n",
    "        if isinstance(attr, str):\n",
    "            res = label_encoder.transform([attr])\n",
    "            return [res[0]]*(rep + 1)\n",
    "        else:\n",
    "            return [0] + [attr]*rep\n",
    "            \n",
    "    return node_attr_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_class(expression):\n",
    "    # Will it be the same for both datasets ? \n",
    "    le = LabelEncoder()\n",
    "    le.fit(OPERATIONS+FUNCTIONS+ATOMICS+VARIABLE_ALPHABET)\n",
    "    class MathExpressionDataset(InMemoryDataset):\n",
    "        def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "            super().__init__(root, transform, pre_transform, pre_filter, force_reload=True)\n",
    "            self.load(self.processed_paths[0])\n",
    "            \n",
    "        @property\n",
    "        def raw_file_names(self):\n",
    "            return ['math_datagen.json']\n",
    "\n",
    "        @property\n",
    "        def processed_file_names(self):\n",
    "            return ['data.pt']\n",
    "        \n",
    "\n",
    "        def process(self):\n",
    "            # Read data into huge `Data` list.\n",
    "            data_list = []\n",
    "            for file in self.raw_file_names:\n",
    "                with open(file) as file_handle:\n",
    "                    object_data = json.load(file_handle)\n",
    "                    for comparison in object_data:\n",
    "                        expr = comparison[expression]\n",
    "                        score = comparison[\"score\"]\n",
    "                        geometric_expr = dict_to_geometric_representation(expr, make_node_attribute_encoder(le))\n",
    "                        geometric_expr.y = score #torch.tensor([score],dtype=torch.float32)\n",
    "                        data_list.append(geometric_expr)\n",
    "                        \n",
    "            if self.pre_filter is not None:\n",
    "                data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data_list = [self.pre_transform(data) for data in data_list]\n",
    "            self.save(data_list, self.processed_paths[0])\n",
    "    return MathExpressionDataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpressionPairDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super().__init__()\n",
    "        self.dataset_l = create_dataset_class(\"expr_l\")(root+\"_l\",transform=None, pre_transform=None, pre_filter=None)\n",
    "        self.dataset_r = create_dataset_class(\"expr_r\")(root+\"_r\",transform=None, pre_transform=None, pre_filter=None)\n",
    "        \n",
    "    @property \n",
    "    def num_features(self):\n",
    "        return self.dataset_l.num_features\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_l[idx], self.dataset_r[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG\n",
      "LOG\n",
      "LOG\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "[Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0)]\n",
      "LOG\n",
      "LOG\n",
      "LOG\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "ADD\n",
      "ADD\n",
      "ADD\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "MUL\n",
      "POW\n",
      "POW\n",
      "POW\n",
      "[Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0), Data(x=[1, 4], edge_index=[2, 0], y=8), Data(x=[1, 4], edge_index=[2, 0], y=24), Data(x=[1, 4], edge_index=[2, 0], y=0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "generate_dataset(20,\"math_datagen.json\")\n",
    "dataset = ExpressionPairDataset(root=\"/dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormulaNet(nn.Module):\n",
    "    def __init__(self, hidden_channels: int, embedding_space: int):\n",
    "        super(FormulaNet, self).__init__()\n",
    "        self.dense_1 = Linear(dataset.num_features, dataset.num_features) \n",
    "        self.relu_1 = ReLU()\n",
    "        self.gconv_1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.gconv_2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.gconv_3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.dense_2 = Linear(hidden_channels, embedding_space)\n",
    "    \n",
    "    def forward(self, data: Data):\n",
    "        data.x = self.dense_1(data.x)\n",
    "        data.x = self.relu_1(data.x)\n",
    "        data.x = self.gconv_1(data.x, data.edge_index)\n",
    "        data.x = self.relu_1(data.x)\n",
    "        data.x = self.gconv_2(data.x, data.edge_index)\n",
    "        data.x = self.relu_1(data.x)\n",
    "        data.x = self.gconv_3(data.x, data.edge_index)\n",
    "        data.x = self.relu_1(data.x)\n",
    "        data.x = global_mean_pool(data.x, data.batch)\n",
    "        data.x = F.dropout(data.x, p=0.3,training=self.training)\n",
    "        data.x = self.dense_2(data.x)\n",
    "        return data.x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseFormulaNet(nn.Module):\n",
    "    def __init__(self, hidden_channels, embedding_space):\n",
    "        super(SiameseFormulaNet, self).__init__()\n",
    "        self.formulanet = FormulaNet(hidden_channels, embedding_space)\n",
    "        self.fc = nn.Sequential(\n",
    "            Linear(embedding_space*2, embedding_space),\n",
    "            ReLU(inplace=True),\n",
    "            Linear(embedding_space, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid() # TODO: Only used it for testing purposes, everything is subject to change Okay\n",
    "    \n",
    "\n",
    "    def forward(self, expr_l, expr_r):\n",
    "        embed_l = self.formulanet(expr_l)\n",
    "        embed_l = embed_l.view(embed_l.size()[0], -1)\n",
    "        embed_r = self.formulanet(expr_r)\n",
    "        embed_r = embed_r.view(embed_r.size()[0], -1)\n",
    "        \n",
    "        output = torch.cat((embed_l, embed_r), 1)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(dataset, list(range(TRAIN_SAMPLES)))\n",
    "test_dataset = Subset(dataset, list(range(TRAIN_SAMPLES, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate(data_list):\n",
    "    batchA = Batch.from_data_list([data[0] for data in data_list])\n",
    "    batchB = Batch.from_data_list([data[1] for data in data_list])\n",
    "    return batchA, batchB\n",
    "# NOTE: Type ignore only for collate_fn_t ... make sure it doesn't get in the way of correct typing for the dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate) # type: ignore\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseFormulaNet(32,64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    #print(type(data[0][0]))\n",
    "    print(data[0][0].x.dtype)\n",
    "    #model(data[0][0].to(device), data[0][1].to(device))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
